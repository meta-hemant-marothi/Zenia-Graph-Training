{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850d54a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"I'm happy to help! As a helpful assistant, I support the latest version of LLaMA, which is LLaMA 2. This version brings significant improvements in conversational AI, including better understanding of context, more accurate responses, and a more natural tone. If you have any specific questions or topics you'd like to discuss with me, I'm here to assist you!\", role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY is not set! Check your .env file.\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the latest version of Llama you support?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450eb5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redis and Chroma are two different types of databases used for different purposes. Here's a brief overview of each and how to use them:\n",
      "\n",
      "**Redis**\n",
      "\n",
      "Redis is an in-memory, NoSQL data store that can be used as a database, message broker, and more. It's known for its high performance, low latency, and support for various data structures like strings, hashes, lists, sets, and more.\n",
      "\n",
      "To use Redis, you'll need to:\n",
      "\n",
      "1. **Install Redis**: You can download and install Redis from the official website or use a package manager like Homebrew (on macOS) or apt-get (on Linux).\n",
      "2. **Start the Redis server**: Run the Redis server using the command `redis-server` (or `brew services start redis` on macOS).\n",
      "3. **Connect to Redis**: Use a Redis client like `redis-cli` (command-line interface) or a programming language library (e.g., `redis-py` for Python) to connect to the Redis server.\n",
      "\n",
      "Some basic Redis commands:\n",
      "\n",
      "* `SET key value`: Set a key-value pair.\n",
      "* `GET key`: Get the value associated with a key.\n",
      "* `LPUSH list value`: Add a value to a list.\n",
      "* `LRANGE list 0 -1`: Get all elements in a list.\n",
      "\n",
      "**Chroma**\n",
      "\n",
      "Chroma is a vector database designed for storing and searching large datasets of dense vectors, typically used in machine learning applications like computer vision, natural language processing, and recommender systems.\n",
      "\n",
      "To use Chroma, you'll need to:\n",
      "\n",
      "1. **Install Chroma**: You can install Chroma using pip: `pip install chroma`.\n",
      "2. **Create a Chroma database**: Create a new Chroma database using the `chroma` command-line tool or the Chroma Python API.\n",
      "3. **Index your data**: Prepare your vector data (e.g., embeddings from a machine learning model) and index it in Chroma using the `chroma.index` method.\n",
      "4. **Search and query**: Use the `chroma.query` method to search for similar vectors in the indexed dataset.\n",
      "\n",
      "Some basic Chroma examples (using Python):\n",
      "\n",
      "```python\n",
      "import chroma\n",
      "\n",
      "# Create a Chroma database\n",
      "db = chroma.Chroma()\n",
      "\n",
      "# Index some sample data (e.g., vector embeddings)\n",
      "vectors = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
      "db.index(vectors)\n",
      "\n",
      "# Search for similar vectors\n",
      "query_vector = [1, 2, 3]\n",
      "results = db.query(query_vector, top_k=2)\n",
      "\n",
      "print(results)  # Print the top-K similar vectors\n",
      "```\n",
      "\n",
      "**Using Redis and Chroma together**\n",
      "\n",
      "You can use Redis and Chroma together to leverage the strengths of both databases. For example:\n",
      "\n",
      "* Store metadata or auxiliary data in Redis, while storing vector embeddings in Chroma.\n",
      "* Use Redis as a caching layer for Chroma queries or indexing results.\n",
      "\n",
      "Here's a simple example of using Redis and Chroma together (using Python):\n",
      "\n",
      "```python\n",
      "import redis\n",
      "import chroma\n",
      "\n",
      "# Connect to Redis\n",
      "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
      "\n",
      "# Create a Chroma database\n",
      "db = chroma.Chroma()\n",
      "\n",
      "# Index some sample data (e.g., vector embeddings)\n",
      "vectors = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
      "db.index(vectors)\n",
      "\n",
      "# Store metadata in Redis\n",
      "metadata = {'vector1': 'some metadata'}\n",
      "redis_client.hset('metadata', 'vector1', metadata)\n",
      "\n",
      "# Search for similar vectors in Chroma\n",
      "query_vector = [1, 2, 3]\n",
      "results = db.query(query_vector, top_k=2)\n",
      "\n",
      "# Retrieve metadata from Redis\n",
      "for result in results:\n",
      "    vector_id = result['id']\n",
      "    metadata = redis_client.hget('metadata', vector_id)\n",
      "    print(metadata)  # Print the metadata for each result\n",
      "```\n",
      "\n",
      "Keep in mind that this is just a simple example to illustrate the idea. You'll need to adapt it to your specific use case and requirements."
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How to use redis store and chroma store.\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61740f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
